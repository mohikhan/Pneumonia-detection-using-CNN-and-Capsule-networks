{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7Iq9ifsP1K3tJYiXcVc8N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammad-debug/Pneumonia-Caps/blob/main/MiniProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtWYBX2Nq5Lu"
      },
      "source": [
        "#MINOR PROJECT \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Pneumonia Detection Using Capsule Networks By Mohammad Anas & Mohiudeen Khan\n",
        "\n",
        "Image classification has become one of the main tasks in the field of computer vision technologies. State-of-the-art approaches in automated classification use deep convolutional neural networks (CNNs). However, these networks require a large number of training samples to generalize well. This project investigates the use of capsule networks (CapsNets) as an alternative to CNNs. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-RuU8_uwX3t"
      },
      "source": [
        "# load dependencies\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf   # Using tensorflow 2.0.0\n",
        "from keras import layers, initializers\n",
        "from keras import backend as K\n",
        "from keras import activations\n",
        "from keras import utils\n",
        "from keras.models import Model\n",
        "from keras.layers import *\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
        "from keras import regularizers\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "IMG_SIZE = 299\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqvK39bxQQ_g"
      },
      "source": [
        "#Downloading the dataset and unzip the train and test dataset\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!pip install -q kaggle \n",
        "!mkdir -p ~/.kaggle \n",
        "!cp kaggle.json ~/.kaggle/ \n",
        "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia  ##upload your api key generated from kaggle\n",
        "!unzip /content/chest-xray-pneumonia.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Mn2FzuQgVD"
      },
      "source": [
        "###Image Augmentation\n",
        "Image Augmentation is a very simple, but very powerful tool to help you avoid overfitting your data.\n",
        "We have used keras image preprossessing tool. It doesn't require us to edit our raw images, nor does it amend them for you on-disk. It does it in-memory as it's performing the training, allowing you to experiment without impacting our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgPFoCx3wrY3"
      },
      "source": [
        "# This code block should create an instance of an ImageDataGenerator called datagen \n",
        "def DataGenerator(train_batch, val_batch, IMG_SIZE):\n",
        "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                 rescale=1./255,#rescale factor\n",
        "                                 rotation_range=10,# Degree range for random rotations.\n",
        "                                 horizontal_flip=True,#horizontal flip in images\n",
        "                                 vertical_flip=True)#vertical flip in images\n",
        "\n",
        "    datagen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n",
        "\n",
        "    train_gen = datagen.flow_from_directory('/content/chest_xray/train/',\n",
        "                                            target_size=(IMG_SIZE, IMG_SIZE),\n",
        "                                            color_mode='rgb', \n",
        "                                            class_mode='categorical',\n",
        "                                            batch_size=train_batch)\n",
        "\n",
        "    val_gen = datagen.flow_from_directory('/content/chest_xray/val/', \n",
        "                                          target_size=(IMG_SIZE, IMG_SIZE),\n",
        "                                          color_mode='rgb', \n",
        "                                          class_mode='categorical',\n",
        "                                          batch_size=val_batch)\n",
        "\n",
        "    datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
        "                                 rescale=1./255)\n",
        "    \n",
        "    datagen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n",
        "\n",
        "    test_gen = datagen.flow_from_directory('/content/chest_xray/test/', \n",
        "                                           target_size=(IMG_SIZE, IMG_SIZE),\n",
        "                                           color_mode='rgb', \n",
        "                                           class_mode='categorical',\n",
        "                                           shuffle=False)\n",
        "    \n",
        "    return train_gen, val_gen, test_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_wr5tZhSBte"
      },
      "source": [
        "### Some setup functions for our Model\n",
        "\n",
        "- Squash function is used to normalize the magnitude of vectors, rather than the scalar elements themselves. The outputs from these squash functions tell us how to route data through various capsules that are trained to learn different concepts. \n",
        "- The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution\n",
        "- Margin loss function representing the price paid for inaccuracy of predictions in our classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT2G8NU2wv8b"
      },
      "source": [
        "# the squashing function.\n",
        "\"\"\"\n",
        "The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
        ":param vectors: some vectors to be squashed, N-dim tensor\n",
        ":param axis: the axis to squash\n",
        ":return: a Tensor with same shape as input vectors\n",
        "\"\"\"\n",
        "def squash(x, axis=-1):\n",
        "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
        "    scale = K.sqrt(s_squared_norm) / (0.5 + s_squared_norm)\n",
        "    return scale * x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWvGDTicw3lM"
      },
      "source": [
        "# define our own softmax function instead of K.softmax\n",
        "# because K.softmax can not specify axis.\n",
        "def softmax(x, axis=-1):\n",
        "    ex = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "    return ex / K.sum(ex, axis=axis, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVz7WaGgw8dz"
      },
      "source": [
        "def margin_loss(y_true, y_pred):\n",
        "    lamb, margin = 0.5, 0.1 #default lambda 0.5 - but test with lambda with 0.9 - 0.1\n",
        "    return K.sum(y_true * K.square(K.relu(1 - margin - y_pred)) + lamb * (\n",
        "        1 - y_true) * K.square(K.relu(y_pred - margin)), axis=-1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzbzI5AXVU3s"
      },
      "source": [
        "##Class setup for Capsule Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_6lMHPhxIn7"
      },
      "source": [
        "def caps_batch_dot(x, y):\n",
        "    x = K.expand_dims(x, 2)\n",
        "    if K.int_shape(x)[3] is not None:\n",
        "        y = K.permute_dimensions(y, (0, 1, 3, 2))\n",
        "    o = tf.matmul(x, y)\n",
        "    return K.squeeze(o, 2)\n",
        "\n",
        "class Capsule(Layer):\n",
        "    \"\"\"\n",
        "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
        "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
        "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_capsule] and output shape = \\\n",
        "    [None, num_capsule, dim_capsule]. For Dense Layer, input_dim_capsule = dim_capsule = 1.\n",
        "    \n",
        "    :param num_capsule: number of capsules in this layer\n",
        "    :param dim_capsule: dimension of the output vectors of the capsules in this layer\n",
        "    :param routings: number of iterations for the routing algorithm\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_capsule,\n",
        "                 dim_capsule,\n",
        "                 routings=3,\n",
        "                 share_weights=True,\n",
        "                 activation='squash',\n",
        "                 **kwargs):\n",
        "        super(Capsule, self).__init__(**kwargs)\n",
        "        self.num_capsule = num_capsule\n",
        "        self.dim_capsule = dim_capsule\n",
        "        self.routings = routings\n",
        "        self.share_weights = share_weights\n",
        "        if activation == 'squash':\n",
        "            self.activation = squash\n",
        "        else:\n",
        "            self.activation = activations.get(activation)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim_capsule = input_shape[-1]\n",
        "        if self.share_weights:\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(1, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "        else:\n",
        "            if input_shape[-2] is None:\n",
        "                raise ValueError(\"Input Shape must be defied if weights not shared.\")\n",
        "            input_num_capsule = input_shape[-2]\n",
        "            self.kernel = self.add_weight(\n",
        "                name='capsule_kernel',\n",
        "                shape=(input_num_capsule, input_dim_capsule,\n",
        "                       self.num_capsule * self.dim_capsule),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Following the routing algorithm from Hinton's paper,\n",
        "        but replace b = b + <u,v> with b = <u,v>.\n",
        "        This change can improve the feature representation of Capsule.\n",
        "        However, you can replace\n",
        "            b = K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        with\n",
        "            b += K.batch_dot(outputs, hat_inputs, [2, 3])\n",
        "        to realize a standard routing.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.share_weights:\n",
        "            hat_inputs = K.conv1d(inputs, self.kernel)\n",
        "        else:\n",
        "            hat_inputs = K.local_conv1d(inputs, self.kernel, [1], [1])\n",
        "\n",
        "        batch_size = K.shape(inputs)[0]\n",
        "        input_num_capsule = K.shape(inputs)[1]\n",
        "        hat_inputs = K.reshape(hat_inputs,\n",
        "                               (batch_size, input_num_capsule,\n",
        "                                self.num_capsule, self.dim_capsule))\n",
        "        hat_inputs = K.permute_dimensions(hat_inputs, (0, 2, 1, 3))\n",
        "\n",
        "        b = K.zeros_like(hat_inputs[:, :, :, 0])\n",
        "\n",
        "        \n",
        "        # Begin: Routing algorithm ---------------------------------------------------------------------#\n",
        "        # The prior for coupling coefficient, initialized as zeros.\n",
        "        # b.shape = [None, self.num_capsule, self.input_num_capsule].\n",
        "\n",
        "        for i in range(self.routings):\n",
        "            c = softmax(b, 1)\n",
        "            o = self.activation(caps_batch_dot(c, hat_inputs))\n",
        "            if i < self.routings - 1:\n",
        "                b = caps_batch_dot(o, hat_inputs)\n",
        "                if K.backend() == 'theano':\n",
        "                    o = K.sum(o, axis=1)\n",
        "        # End: Routing algorithm -----------------------------------------------------------------------#\n",
        "        return o\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.num_capsule, self.dim_capsule)\n",
        "        \n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'num_capsule': self.num_capsule,\n",
        "            'dim_capsule': self.dim_capsule,\n",
        "            'routings': self.routings\n",
        "        }\n",
        "        base_config = super(Capsule, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0nVTBF-xRJr"
      },
      "source": [
        "#Generating data for training using Image Generator defined above\n",
        "train_batch = 32\n",
        "val_batch = 1\n",
        "\n",
        "train, val, test = DataGenerator(train_batch, val_batch, IMG_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHvO9_P-mbv3"
      },
      "source": [
        "### Importing  VGG16 as our base model (Transfer Learning)\n",
        "A pre-trained model is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. You either use the pretrained model as is or use transfer learning to customize this model to a given task.\n",
        "\n",
        "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5V4pY9p_f2i",
        "outputId": "621e5f6e-513f-4488-decb-817d76e7d861"
      },
      "source": [
        "input_image = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "\n",
        "# A InceptionResNetV2 Conv2D model\n",
        "base_model = VGG16(include_top=False, weights='imagenet', input_tensor=input_image)\n",
        "\n",
        "base_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWXOY8-Bn-UP"
      },
      "source": [
        "##Network I\n",
        "In our first network, our first layer (base layer) is VGG 16 with its layers frozen. After that, we have applied a Global average pooling layer. Then we have a dense layer of 4096 neurons with activation \"Relu\". after that we have a dropout layer with rate 0.5 and finally an output \"softmax\" layer with 2 outcomes. The loss function we used in our network was categorical_crossentropy and optimizer was RMSprop with learning rate=1e-4. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyTea6VLsKz4"
      },
      "source": [
        "####Feature extraction\n",
        " We will freeze the convolutional base created from the previous step and to use as a feature extractor.\n",
        "\n",
        "####Freeze the convolutional base\n",
        "It is important to freeze the convolutional base before we compile and train the model. Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. VGG 16 has many layers, so setting the entire model's trainable flag to False will freeze all of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PBjEN0n_rna",
        "outputId": "a367abe9-f360-442d-bec5-1f6705b34cf6"
      },
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "    print(layer, layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7ff3410e35f8> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff340ef4ef0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff340f8e860> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff708edb080> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708edb4a8> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff340f97fd0> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff708ec8a90> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708ec8320> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708ec0898> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708ee7cf8> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff708ec3fd0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708ec3400> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708ee7da0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708ca82b0> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff708c935c0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708ca8978> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708ca85c0> False\n",
            "<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7ff708cad828> False\n",
            "<tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7ff708cbabe0> False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAYIXJRD_1Q-",
        "outputId": "dc019472-8ba7-42e9-b049-b3a3a6fa855f"
      },
      "source": [
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(4096, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(2, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_image, outputs=output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_11  (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 4096)              2101248   \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 2)                 8194      \n",
            "=================================================================\n",
            "Total params: 16,824,130\n",
            "Trainable params: 2,109,442\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8152pVGJAMj8",
        "outputId": "bcc8463f-0b85-4554-ae05-7060e5f96079"
      },
      "source": [
        "model.fit_generator(train,\n",
        "                    epochs=1,\n",
        "                    validation_data=val, \n",
        "                    validation_steps = len(val.classes)//val_batch,\n",
        "                    steps_per_epoch=(len(train.classes)//train_batch) ) \n",
        "    \n",
        "loss, acc = model.evaluate_generator(test, len(test))\n",
        "\n",
        "print (\"\\n\\n================================\\n\\n\")\n",
        "print (\"Loss: {}\".format(loss))\n",
        "print (\"Accuracy: {0:.2f} %\".format(acc * 100))\n",
        "print (\"\\n\\n================================\\n\\n\")\n",
        "\n",
        "test.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "163/163 [==============================] - 133s 815ms/step - loss: 0.3842 - accuracy: 0.8223 - val_loss: 0.6562 - val_accuracy: 0.6875\n",
            "\n",
            "\n",
            "================================\n",
            "\n",
            "\n",
            "Loss: 0.43011680245399475\n",
            "Accuracy: 79.01 %\n",
            "\n",
            "\n",
            "================================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99e4v21F9kWs"
      },
      "source": [
        "##Network II\n",
        "In our second network, the architecture is almost the same. We have fine tuned our VGG_16 net and the loss function we used here was categorical_crossentropy and optimizer was SGD with learning rate=1e-4 and momentum=0.9. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtPteq6e-Qjf"
      },
      "source": [
        "###Fine Tuning\n",
        "In the feature extraction, we were only training a few layers on top of an VGG_16 base model. The weights of the pre-trained network were not updated during training.\n",
        "\n",
        "One way to increase performance even further is to train (or \"fine-tune\") the weights of the top layers of the pre-trained model alongside the training of the classifier we added. The training process will force the weights to be tuned from generic feature maps to features associated specifically with the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3mqbu2exryW"
      },
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "    if i < 15:\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSymiSo1xwaU"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=lr, momentum=0.9), metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAq_RPuox0p-"
      },
      "source": [
        "history=model.fit(train,\n",
        "          epochs=10,\n",
        "          validation_data=val, \n",
        "          validation_steps = len(val.classes)//val_batch,\n",
        "          steps_per_epoch=(len(train.classes)//train_batch) * 2) \n",
        "    \n",
        "loss, acc = model.evaluate_generator(test, len(test))\n",
        "\n",
        "\n",
        "\n",
        "test.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM3xINnfUKNO",
        "outputId": "64450306-bc11-44fb-8f5e-bfc411fd891c"
      },
      "source": [
        "print (\"\\n\\n================================\\n\\n\")\n",
        "print (\"Loss: {}\".format(loss))\n",
        "print (\"Accuracy: {0:.2f} %\".format(acc * 100))\n",
        "print (\"\\n\\n================================\\n\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "================================\n",
            "\n",
            "\n",
            "Loss: 0.756589949131012\n",
            "Accuracy: 79.49 %\n",
            "\n",
            "\n",
            "================================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We6nJC1__Q_h"
      },
      "source": [
        "##Network III\n",
        "In our third network, our first layer (base layer) is VGG 16. After that, we have a convolutional 2D layer of 256 filters with stride=1,9x9 kernel size and activation function=\"Relu\". Now we finally have our Capsule Network comprising followed by lambda ouput\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsG3nLpZx5ei",
        "outputId": "43d938cb-429b-49d5-9e45-7145d22aeb46"
      },
      "source": [
        "output = Conv2D(256, kernel_size=(9, 9), strides=(1, 1), activation='relu')(base_model.get_layer(name='block5_pool').output)\n",
        "\n",
        "x = Reshape((-1, 256))(output)\n",
        "capsule = Capsule(2, 16, 4, True)(x)\n",
        "output = Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(capsule)\n",
        "model = Model(inputs=input_image, outputs=output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 299, 299, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 299, 299, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 299, 299, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 149, 149, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 149, 149, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 149, 149, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 74, 74, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 74, 74, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 74, 74, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 37, 37, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 37, 37, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 37, 37, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 18, 18, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 1, 1, 256)         10617088  \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 1, 256)            0         \n",
            "_________________________________________________________________\n",
            "capsule (Capsule)            (None, 2, 16)             8192      \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 25,339,968\n",
            "Trainable params: 25,339,968\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkJ_Pyjpx_Kn"
      },
      "source": [
        "lr=1e-4\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"weights.h5\", \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             save_weights_only=False, \n",
        "                             mode='min')\n",
        "\n",
        "early = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', restore_best_weights=True)\n",
        "\n",
        "callback_list = [checkpoint, early]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nprkIHBPuz86",
        "outputId": "db6138e2-9d34-42b7-96f4-d3126d165d60"
      },
      "source": [
        "epochs=100\n",
        "\n",
        "model.compile(loss=margin_loss, optimizer=SGD(lr=lr, momentum=0.9), metrics=['accuracy'])\n",
        "\n",
        "\n",
        "history=model.fit_generator(train,\n",
        "          epochs=epochs,\n",
        "          validation_data=val, \n",
        "          validation_steps = len(val.classes)//val_batch,\n",
        "          steps_per_epoch=len(train.classes)//train_batch,\n",
        "          callbacks=callback_list\n",
        "          )\n",
        "    \n",
        "loss, acc = model.evaluate_generator(test, len(test))\n",
        "\n",
        "print (\"\\n\\n================================\\n\\n\")\n",
        "print (\"Loss: {}\".format(loss))\n",
        "print (\"Accuracy: {0:.2f} %\".format(acc * 100))\n",
        "print (\"\\n\\n================================\\n\\n\")\n",
        "\n",
        "\n",
        "test.reset()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0402 - accuracy: 0.9404\n",
            "Epoch 00001: val_loss improved from inf to 0.18228, saving model to weights.h5\n",
            "163/163 [==============================] - 136s 837ms/step - loss: 0.0402 - accuracy: 0.9404 - val_loss: 0.1823 - val_accuracy: 0.7500\n",
            "Epoch 2/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9666\n",
            "Epoch 00002: val_loss improved from 0.18228 to 0.16109, saving model to weights.h5\n",
            "163/163 [==============================] - 136s 835ms/step - loss: 0.0246 - accuracy: 0.9666 - val_loss: 0.1611 - val_accuracy: 0.7500\n",
            "Epoch 3/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9714\n",
            "Epoch 00003: val_loss did not improve from 0.16109\n",
            "163/163 [==============================] - 137s 840ms/step - loss: 0.0202 - accuracy: 0.9714 - val_loss: 0.2012 - val_accuracy: 0.7500\n",
            "Epoch 4/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9755\n",
            "Epoch 00004: val_loss improved from 0.16109 to 0.09940, saving model to weights.h5\n",
            "163/163 [==============================] - 137s 841ms/step - loss: 0.0185 - accuracy: 0.9755 - val_loss: 0.0994 - val_accuracy: 0.8750\n",
            "Epoch 5/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0143 - accuracy: 0.9789\n",
            "Epoch 00005: val_loss improved from 0.09940 to 0.05776, saving model to weights.h5\n",
            "163/163 [==============================] - 138s 847ms/step - loss: 0.0143 - accuracy: 0.9789 - val_loss: 0.0578 - val_accuracy: 0.8125\n",
            "Epoch 6/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9745\n",
            "Epoch 00006: val_loss improved from 0.05776 to 0.04680, saving model to weights.h5\n",
            "163/163 [==============================] - 137s 840ms/step - loss: 0.0165 - accuracy: 0.9745 - val_loss: 0.0468 - val_accuracy: 0.8750\n",
            "Epoch 7/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9778\n",
            "Epoch 00007: val_loss improved from 0.04680 to 0.03037, saving model to weights.h5\n",
            "163/163 [==============================] - 137s 844ms/step - loss: 0.0149 - accuracy: 0.9778 - val_loss: 0.0304 - val_accuracy: 1.0000\n",
            "Epoch 8/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9778\n",
            "Epoch 00008: val_loss did not improve from 0.03037\n",
            "163/163 [==============================] - 137s 840ms/step - loss: 0.0152 - accuracy: 0.9778 - val_loss: 0.2341 - val_accuracy: 0.6875\n",
            "Epoch 9/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9772\n",
            "Epoch 00009: val_loss did not improve from 0.03037\n",
            "163/163 [==============================] - 137s 841ms/step - loss: 0.0155 - accuracy: 0.9772 - val_loss: 0.0661 - val_accuracy: 0.9375\n",
            "Epoch 10/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0122 - accuracy: 0.9824\n",
            "Epoch 00010: val_loss did not improve from 0.03037\n",
            "163/163 [==============================] - 138s 846ms/step - loss: 0.0122 - accuracy: 0.9824 - val_loss: 0.1122 - val_accuracy: 0.8750\n",
            "Epoch 11/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9778\n",
            "Epoch 00011: val_loss did not improve from 0.03037\n",
            "163/163 [==============================] - 139s 850ms/step - loss: 0.0153 - accuracy: 0.9778 - val_loss: 0.1510 - val_accuracy: 0.7500\n",
            "Epoch 12/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9850\n",
            "Epoch 00012: val_loss did not improve from 0.03037\n",
            "163/163 [==============================] - 139s 854ms/step - loss: 0.0114 - accuracy: 0.9850 - val_loss: 0.0506 - val_accuracy: 0.8750\n",
            "Epoch 13/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9839\n",
            "Epoch 00013: val_loss improved from 0.03037 to 0.02271, saving model to weights.h5\n",
            "163/163 [==============================] - 139s 853ms/step - loss: 0.0126 - accuracy: 0.9839 - val_loss: 0.0227 - val_accuracy: 0.9375\n",
            "Epoch 14/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9812\n",
            "Epoch 00014: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 140s 860ms/step - loss: 0.0130 - accuracy: 0.9812 - val_loss: 0.1750 - val_accuracy: 0.7500\n",
            "Epoch 15/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0124 - accuracy: 0.9839\n",
            "Epoch 00015: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 140s 857ms/step - loss: 0.0124 - accuracy: 0.9839 - val_loss: 0.1140 - val_accuracy: 0.8750\n",
            "Epoch 16/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.9831\n",
            "Epoch 00016: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 140s 859ms/step - loss: 0.0121 - accuracy: 0.9831 - val_loss: 0.1031 - val_accuracy: 0.7500\n",
            "Epoch 17/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0115 - accuracy: 0.9826\n",
            "Epoch 00017: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 141s 868ms/step - loss: 0.0115 - accuracy: 0.9826 - val_loss: 0.1863 - val_accuracy: 0.7500\n",
            "Epoch 18/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0112 - accuracy: 0.9839\n",
            "Epoch 00018: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 141s 862ms/step - loss: 0.0112 - accuracy: 0.9839 - val_loss: 0.0309 - val_accuracy: 0.9375\n",
            "Epoch 19/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9877\n",
            "Epoch 00019: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 141s 867ms/step - loss: 0.0082 - accuracy: 0.9877 - val_loss: 0.0849 - val_accuracy: 0.8750\n",
            "Epoch 20/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9843\n",
            "Epoch 00020: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 141s 865ms/step - loss: 0.0104 - accuracy: 0.9843 - val_loss: 0.0397 - val_accuracy: 0.9375\n",
            "Epoch 21/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0107 - accuracy: 0.9860\n",
            "Epoch 00021: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 142s 869ms/step - loss: 0.0107 - accuracy: 0.9860 - val_loss: 0.0382 - val_accuracy: 0.9375\n",
            "Epoch 22/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0101 - accuracy: 0.9854\n",
            "Epoch 00022: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 142s 869ms/step - loss: 0.0101 - accuracy: 0.9854 - val_loss: 0.0499 - val_accuracy: 0.9375\n",
            "Epoch 23/100\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0077 - accuracy: 0.9889\n",
            "Epoch 00023: val_loss did not improve from 0.02271\n",
            "163/163 [==============================] - 142s 869ms/step - loss: 0.0077 - accuracy: 0.9889 - val_loss: 0.0697 - val_accuracy: 0.9375\n",
            "\n",
            "\n",
            "================================\n",
            "\n",
            "\n",
            "Loss: 0.08784601837396622\n",
            "Accuracy: 88.94 %\n",
            "\n",
            "\n",
            "================================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}